1.move file form onefolder to respective folders

for file in 'ls *.txt'
	do	
		foldername='echo $file | awk -F. '{print $1}'
		echo "$foldername"
		if[-d $foldername]
		then		
			rm -R $foldername
		fi
			mkdir $foldername
			mv $file $foldername
	done



2.append current date to all log files which has extension.log

for file in 'ls *.log.1'
	do	
		foldername='echo $file | awk -F. '{print $1}'
		echo $foldername
		if[-d $foldername]
		then
			rm -R $foldername
		fi	
		 myvalue=$foldername-"date+""%d%m%y"".log
		 mv $file $folder
		 echo $myvalue
done




3.check if folder exist or not if its not present creat it

#!/bin/bash -x

echo "enter a name"~/
read name
if[-d $name]
 then
	echo"folder already exist"
else 
	mkdir $name
fi


4. data analysis/manupilation

	1.. print employeename and totalpay who has base pay greater than 100000
	--->cat data.csv | awk '$4>100000 {print $2""$7}'
	2..What is the aggregate TotalPay of employees whose jobtitle is CAPTAIN
	--->cat data.csv | grep -i captain | awk '{sum+=$7}END{print sum }'
	3..Print JobTitle and Overtimepay who has Overtimepay is between 7000 and 10000
	--->cat data.csv | awk $5>7000 && $5<10000 '{ print $3""$5}'
	4..Print average BasePay
	--->cat data.csv | grep -i captain | awk '{sum+=$7}END{print sum/(NR-1) }'



5..Print list of last 4 frequently access unique urls at particular hours from
access.log

cat access.log | awk '{print $4,$11}' | sort -n | uniq -c |head -4



6..Print list of web response code count in the unique sorted order at specific hours
cat access.log | awk '{print $9}' | sort | uniq -c | head -4


7..Print list of last 10 unique sorted client IP from /var/log/httpd/access.log 
cat access.log | awk '{print $1 }'| sort -n | uniq -c | head -10




new

if [[ $m >3 && $m <6]] then echo $day $m else echo 'false' fi
echo 'enter number 1 to 12 for ex:1:january,2:feb'
Nibedita Jena6:51 PM
$y
 %
Nibedita Jena6:52 PM
read a
a=1
if($a==1)
Nibedita Jena6:53 PM
else echo 'please enter number not text'